{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境配置"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 此为在线运行平台配置python3.9的指南，如在其他环境平台运行案例，请根据实际情况修改如下代码"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步：设置python版本为3.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "!/home/ma-user/anaconda3/bin/conda create -n python-3.9.0 python=3.9.0 -y --override-channels --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n",
    "!/home/ma-user/anaconda3/envs/python-3.9.0/bin/pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "data = {\n",
    "   \"display_name\": \"python-3.9.0\",\n",
    "   \"env\": {\n",
    "      \"PATH\": \"/home/ma-user/anaconda3/envs/python-3.9.0/bin:/home/ma-user/anaconda3/envs/python-3.7.10/bin:/modelarts/authoring/notebook-conda/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/ma-user/modelarts/ma-cli/bin:/home/ma-user/modelarts/ma-cli/bin\"\n",
    "   },\n",
    "   \"language\": \"python\",\n",
    "   \"argv\": [\n",
    "      \"/home/ma-user/anaconda3/envs/python-3.9.0/bin/python\",\n",
    "      \"-m\",\n",
    "      \"ipykernel\",\n",
    "      \"-f\",\n",
    "      \"{connection_file}\"\n",
    "   ]\n",
    "}\n",
    "\n",
    "if not os.path.exists(\"/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/\"):\n",
    "    os.mkdir(\"/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/\")\n",
    "\n",
    "with open('/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/kernel.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注：以上代码运行完成后，需要重新设置kernel为python-3.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://mindspore-demo.obs.cn-north-4.myhuaweicloud.com/imgs/ai-gallery/change-kernel.PNG\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二步：安装MindSpore框架和MindNLP套件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mindspore官网提供了不同的mindspore版本，可以根据自己的操作系统和Python版本，安装不同版本的mindspore\n",
    "\n",
    "\n",
    "https://www.mindspore.cn/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.5.0/MindSpore/unified/x86_64/mindspore-2.5.0-cp39-cp39-linux_x86_64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，这里如果通过pip安装mindnlp的话，需要参考链接对mindnlp/core/nn/modules/module.py进行Files changed所示的修改，确保loss正确下降，链接教程：https://github.com/mindspore-lab/mindnlp/pull/2007/files\n",
    "可以通过git clone下载最新的mindnlp，然后将下载的最新的mindnlp版本替换到你环境中安装mindnlp的位置，一般是/home/user/miniconda3/envs/yourenv/lib/python3.9/site-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mindnlp==0.4.0 -i https://mirrors.aliyun.com/pypi/simple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMA微调推理全流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLaMA（Large Language Model Meta AI）是由Meta（前Facebook）推出的一系列开源大规模语言模型，意味着大模型应用进入了“免费时代”，初创公司也能够以低廉的价格来创建类似ChatGPT这样的聊天机器人。 LLaMA 专注于自然语言处理任务，其结构基于Transformer架构，但是进行了改进，比如引入更高效的注意力机制和更紧凑的模型设计，例如使用SwiGLU激活函数 使用RoPE位置编码等。\n",
    "\n",
    "LLaMA的模型参数规模设计更为精细，以更少的参数实现了与更大模型相当的性能。例如，LLaMA-7B模型的性能可以与175B参数规模的GPT-3媲美。模型在推理过程中采用了多查询注意力（Multi-Query Attention, MQA）机制，改进了传统多头注意力的查询方式，将多个注意力头的查询统一为单个查询头，从而显著减少了推理时间和显存需求，提升了效率。适用于文本生成、问答、翻译等任务。例如，在文本生成任务中，LLaMA可以生成高质量的文章段落，其轻量化设计降低了硬件需求，使研究者和开发者更容易使用高性能的语言模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入必要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "import mindspore.dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将模式设置为动态图模式（PYNATIVE_MODE），并指定设备目标为Ascend芯片\n",
    "ms.set_context(mode=ms.PYNATIVE_MODE, device_target=\"Ascend\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#指定模型路径\n",
    "base_model_path = \"/home/jiangna1/.mindnlp/model/hfl/chinese-llama-2-1.3b\" #中文模型，这里我提前下载到了本地减少下载时间\n",
    "# base_model_path = \"NousResearch/Hermes-3-Llama-3.2-3B\" #英文模型\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集\n",
    "\n",
    "这里提供两份用于微调的数据集，分别用于中文模型和英文模型，其中中文模型为hfl/chinese-llama-2-1.3b，参数量为1.3B，英文模型为NousResearch/Hermes-3-Llama-3.2-3B，参数量为3.2B，用户可以根据自己的配置或期望训练时长自行选择。\n",
    "\n",
    "数据来源皆为hugging face公开用于微调的数据集，其中中文数据集来源为弱智吧，数据格式为：\n",
    "\n",
    "  {\"instruction\": \"只剩一个心脏了还能活吗？\",\n",
    "\n",
    "    \"output\": \"能，人本来就只有一个心脏。\"},\n",
    "\n",
    "  {\"instruction\": \"爸爸再婚，我是不是就有了个新娘？\",\n",
    "\n",
    "    \"output\": \"不是的，你有了一个继母。\\\"新娘\\\"是指新婚的女方，而你爸爸再婚，他的新婚妻子对你来说是继母。\"}\n",
    "\n",
    "\n",
    "英文数据来源为Alpaca,数据格式为：\n",
    "\n",
    "  {\"instruction\": \"Give three tips for staying healthy.\",\n",
    "\n",
    "    \"input\": \"\",\n",
    "\n",
    "    \"output\": \"1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.\"},\n",
    "\n",
    "  {\"instruction\": \"What are the three primary colors?\",\n",
    "\n",
    "    \"input\": \"\",\n",
    "\n",
    "    \"output\": \"The three primary colors are red, blue, and yellow.\"}\n",
    "\n",
    "\n",
    "以下教程同时包括包括中文和英文模型的微调教程为例，其中英文模型微调效果更好，但因为时间关系，本模型展示主要以小规模的中文为例，可以自行根据自己的需求修改数据来源和模型。\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据加载和数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新建 tokenize_function 函数用于数据预处理，具体内容可见下面代码注释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example, tokenizer):\n",
    "    instruction = example.get(\"instruction\", \"\")\n",
    "    input_text = example.get(\"input\", \"\")\n",
    "    output = example.get(\"output\", \"\")\n",
    "    # prompt\n",
    "    if input_text:\n",
    "        prompt = f\"User: {instruction} {input_text}\\nAssistant: {output}\"\n",
    "    else:\n",
    "        prompt = f\"User: {instruction}\\nAssistant: {output}\"\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    input_ids = np.array(tokenized[\"input_ids\"], dtype=np.int32)\n",
    "\n",
    "    # Handle label\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    labels = np.array(\n",
    "        [-100 if token_id == pad_token_id else token_id for token_id in input_ids], dtype=np.int32\n",
    "    )\n",
    "    return input_ids, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据来源如下，为了避免网络问题，建议先下载到本地\n",
    "\n",
    "https://huggingface.co/datasets/LooksJuicy/ruozhiba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_path = \"/home/jiangna1/mindnlp_llama_all/alpaca_data.json\" #英文数据集\n",
    "data_path = \"/home/jiangna1/mindnlp_llama_all/chinese_data.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看数据具体内容，该数据只包括instruction和output两列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从指定路径加载预训练的分词器，该分词器能将输入文本分割成模型可处理的词元。接着，将填充标记设置为结束标记，这样在处理不同长度的文本序列时，用结束标记来填充额外位置，避免引入额外特殊标记，减少模型学习负担。最后，设置填充方向为右侧，使文本在右侧添加填充标记达到统一长度，维持文本原始顺序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.encode(' ;')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据分为训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_generator(dataset, tokenizer):\n",
    "    for item in dataset:\n",
    "        yield tokenize_function(item, tokenizer)\n",
    "        \n",
    "split_ratio = 0.9\n",
    "split_index = int(len(data) * split_ratio)\n",
    "train_data, val_data = data[:split_index], data[split_index:]\n",
    "\n",
    "train_dataset = ds.GeneratorDataset(\n",
    "    source=lambda: data_generator(train_data, tokenizer), \n",
    "    column_names=[\"input_ids\", \"labels\"]\n",
    ")\n",
    "\n",
    "eval_dataset = ds.GeneratorDataset(\n",
    "    source=lambda: data_generator(val_data, tokenizer), \n",
    "    column_names=[\"input_ids\", \"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看处理后的数据，tokenizer 将输入的文本（prompt）拆分为词片段（tokens），然后将每个词片段映射为对应的 token ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(train_dataset.create_dict_iterator()):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"Sample {i}: Input IDs: {sample['input_ids'][:10]}\") \n",
    "    print(f\"Sample {i}: Labels: {sample['labels'][:10]}\\n\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lora指令微调"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定微调结果输出路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 指定输出路径\n",
    "peft_output_dir = \"/home/jiangna1/mindnlp_llama_all/pert_model_Chinese\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载基座模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.transformers import AutoModelForCausalLM,  GenerationConfig\n",
    "\n",
    "ms_base_model = AutoModelForCausalLM.from_pretrained(base_model_path, ms_dtype=ms.float16)\n",
    "ms_base_model.generation_config = GenerationConfig.from_pretrained(base_model_path)\n",
    "ms_base_model.generation_config.pad_token_id = ms_base_model.generation_config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#修改精度，会使训练变慢，但是训练loss下降效果会变好\n",
    "for name, param in ms_base_model.parameters_and_names():\n",
    "    param.set_dtype(ms.float32) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分代码的主要作用是创建一个 LoRA的配置对象 ms_config，大语言模型进行微调时，LoRA 是一种高效的参数微调方法，通过在预训练模型的基础上添加低秩矩阵来减少需要训练的参数数量，从而提高微调效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mindnlp.peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
    "\n",
    "ms_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,#微调任务的类型\n",
    "    #指定需要应用 LoRA 调整的目标模块\n",
    "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    inference_mode=False,  \n",
    "    r=8,                   \n",
    "    lora_alpha=32,         \n",
    "    lora_dropout=0.1       \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于给定的基础模型和 LoRA 配置创建一个可进行参数高效微调的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_model = get_peft_model(ms_base_model, ms_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练参数的设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.engine import TrainingArguments, Trainer\n",
    "\n",
    "num_train_epochs = 70\n",
    "fp16 = True\n",
    "overwrite_output_dir = True\n",
    "per_device_train_batch_size = 16\n",
    "per_device_eval_batch_size = 32\n",
    "gradient_accumulation_steps = 16\n",
    "gradient_checkpointing = True\n",
    "evaluation_strategy = \"steps\"\n",
    "learning_rate = 1e-5\n",
    "lr_scheduler_type = \"cosine\" \n",
    "weight_decay = 0.01\n",
    "warmup_ratio = 0.1\n",
    "max_grad_norm = 0.3\n",
    "group_by_length = False \n",
    "auto_find_batch_size = False\n",
    "save_steps = 50 \n",
    "logging_strategy = \"steps\"\n",
    "logging_steps = 150                \n",
    "load_best_model_at_end = True \n",
    "packing = False\n",
    "save_total_limit = 3\n",
    "neftune_noise_alpha = 5 \n",
    "eval_steps = 10\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=peft_output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    load_best_model_at_end=load_best_model_at_end,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    evaluation_strategy=evaluation_strategy,\n",
    "    eval_steps=eval_steps,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    auto_find_batch_size=auto_find_batch_size,\n",
    "    save_total_limit=save_total_limit,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    save_steps=save_steps,\n",
    "    logging_strategy=logging_strategy,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=ms_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=training_arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in trainer.model.parameters_and_names():\n",
    "    param.set_dtype(ms.float16)\n",
    "trainer.model.save_pretrained(peft_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后存储还是存储为 16 位浮点数，保存训练后的 LoRA 模型保存到指定的输出目录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，lora微调后保存的并不是完整的参数，在推理时，需要将保存的 LoRA 参数加载到原预训练模型中，合并后得到完整的模型，然后进行推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用PeftModel进行配置和参数合并，最后将模型设置为评估模式，进行推理任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将 LoRA微调后的参数加载到预训练模型中\n",
    "from mindnlp.peft import PeftModel\n",
    "model = PeftModel.from_pretrained(ms_base_model, peft_output_dir)\n",
    "model = model.merge_and_unload()\n",
    "print('model merge succeeded')\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个函数，用于根据用户输入的问题生成相应的回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "\n",
    "def generate_response(question, model, tokenizer, max_length=256):\n",
    "    prompt = f\"以下是用户和助手之间的问答。\\n问：{question}\\n答：\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"ms\", padding=True, truncation=True, max_length=512)\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        # temperature=0.7,\n",
    "        # top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        no_repeat_ngram_size=3,\n",
    "        max_length=max_length,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    response = response.split(\"Answer：\")[-1].strip()\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"如何保持清醒?\"\n",
    "response = generate_response(question, model, tokenizer)\n",
    "\n",
    "print(f\"User: {question}\")\n",
    "print(f\"LLAMA: {response}\")"
   ]
  }
 ],
 "metadata": {
  "AIGalleryInfo": {
   "item_id": "5443b528-0dd5-4909-ac4f-1c9cf839e2aa"
  },
  "flavorInfo": {
   "architecture": "X86_64",
   "category": "GPU"
  },
  "imageInfo": {
   "id": "e1a07296-22a8-4f05-8bc8-e936c8e54202",
   "name": "mindspore1.7.0-cuda10.1-py3.7-ubuntu18.04"
  },
  "kernelspec": {
   "display_name": "ms39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
