{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境配置"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 此为在线运行平台配置python3.9的指南，如在其他环境平台运行案例，请根据实际情况修改如下代码"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步：设置python版本为3.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "!/home/ma-user/anaconda3/bin/conda create -n python-3.9.0 python=3.9.0 -y --override-channels --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n",
    "!/home/ma-user/anaconda3/envs/python-3.9.0/bin/pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "data = {\n",
    "   \"display_name\": \"python-3.9.0\",\n",
    "   \"env\": {\n",
    "      \"PATH\": \"/home/ma-user/anaconda3/envs/python-3.9.0/bin:/home/ma-user/anaconda3/envs/python-3.7.10/bin:/modelarts/authoring/notebook-conda/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/ma-user/modelarts/ma-cli/bin:/home/ma-user/modelarts/ma-cli/bin\"\n",
    "   },\n",
    "   \"language\": \"python\",\n",
    "   \"argv\": [\n",
    "      \"/home/ma-user/anaconda3/envs/python-3.9.0/bin/python\",\n",
    "      \"-m\",\n",
    "      \"ipykernel\",\n",
    "      \"-f\",\n",
    "      \"{connection_file}\"\n",
    "   ]\n",
    "}\n",
    "\n",
    "if not os.path.exists(\"/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/\"):\n",
    "    os.mkdir(\"/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/\")\n",
    "\n",
    "with open('/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/kernel.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注：以上代码运行完成后，需要重新设置kernel为python-3.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://mindspore-demo.obs.cn-north-4.myhuaweicloud.com/imgs/ai-gallery/change-kernel.PNG\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二步：安装MindSpore框架和MindNLP套件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mindspore官网提供了不同的mindspore版本，可以根据自己的操作系统和Python版本，安装不同版本的mindspore\n",
    "\n",
    "\n",
    "https://www.mindspore.cn/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.5.0/MindSpore/unified/x86_64/mindspore-2.5.0-cp39-cp39-linux_x86_64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mindnlp==0.4.0 -i https://mirrors.aliyun.com/pypi/simple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qwen微调推理全流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qwen是由阿里巴巴云研发的大型语言模型。Qwen系列模型在多个基准测试中展现了卓越的性能，包括但不限于语言理解、数学计算、代码编写等方面。特别是在中文相关的任务上，其表现尤为突出，超过了其他一些知名的开源模型。Qwen在设计时充分考虑了中文用户的需求，具有强大的中文处理能力。它不仅能够理解和生成高质量的中文文本，还针对中文的语言特点进行了优化。在训练数据方面更侧重中文语料，针对中文nlp任务进行了优化，以及在推理方面表现出很强的能力。Qwen不仅仅局限于单一的应用领域，而是覆盖了从语言处理到视觉智能等多个方面，为开发者提供了广泛的应用场景。无论是AI绘画、AI写作还是编程辅助，Qwen都能提供有效的支持。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入必要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "import mindspore.dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将模式设置为动态图模式（PYNATIVE_MODE），并指定设备目标为Ascend芯片\n",
    "ms.set_context(mode=ms.PYNATIVE_MODE, device_target=\"Ascend\",device_id=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#指定模型路径\n",
    "base_model_path = \"/home/liangdeqi/liudengjin/Qwen2.5-3B/\" #中文模型，这里我提前下载到了本地减少下载时间\n",
    "# base_model_path = \"NousResearch/Hermes-3-Llama-3.2-3B\" #英文模型\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集\n",
    "\n",
    "这里提供两份用于微调的数据集，分别用于中文模型和英文模型，其中中文模型为hfl/chinese-llama-2-1.3b，参数量为1.3B，英文模型为NousResearch/Hermes-3-Llama-3.2-3B，参数量为3.2B，用户可以根据自己的配置或期望训练时长自行选择。\n",
    "\n",
    "数据来源皆为hugging face公开用于微调的数据集，其中中文数据集来源为弱智吧，数据格式为：\n",
    "\n",
    "  {\"instruction\": \"只剩一个心脏了还能活吗？\",\n",
    "\n",
    "    \"output\": \"能，人本来就只有一个心脏。\"},\n",
    "\n",
    "  {\"instruction\": \"爸爸再婚，我是不是就有了个新娘？\",\n",
    "\n",
    "    \"output\": \"不是的，你有了一个继母。\\\"新娘\\\"是指新婚的女方，而你爸爸再婚，他的新婚妻子对你来说是继母。\"}\n",
    "\n",
    "\n",
    "英文数据来源为Alpaca,数据格式为：\n",
    "\n",
    "  {\"instruction\": \"Give three tips for staying healthy.\",\n",
    "\n",
    "    \"input\": \"\",\n",
    "\n",
    "    \"output\": \"1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.\"},\n",
    "\n",
    "  {\"instruction\": \"What are the three primary colors?\",\n",
    "\n",
    "    \"input\": \"\",\n",
    "\n",
    "    \"output\": \"The three primary colors are red, blue, and yellow.\"}\n",
    "\n",
    "\n",
    "以下教程同时包括包括中文和英文模型的微调教程为例，其中英文模型微调效果更好，但因为时间关系，本模型展示主要以小规模的中文为例，可以自行根据自己的需求修改数据来源和模型。\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据加载和数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新建 tokenize_function 函数用于数据预处理，具体内容可见下面代码注释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example, tokenizer):\n",
    "    instruction = example.get(\"instruction\", \"\")\n",
    "    input_text = example.get(\"input\", \"\")\n",
    "    output = example.get(\"output\", \"\")\n",
    "    # prompt\n",
    "    if input_text:\n",
    "        prompt = f\"User: {instruction} {input_text}\\nAssistant: {output}\"\n",
    "    else:\n",
    "        prompt = f\"User: {instruction}\\nAssistant: {output}\"\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    input_ids = np.array(tokenized[\"input_ids\"], dtype=np.int32)\n",
    "\n",
    "    # Handle label\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    labels = np.array(\n",
    "        [-100 if token_id == pad_token_id else token_id for token_id in input_ids], dtype=np.int32\n",
    "    )\n",
    "    return input_ids, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据来源如下，为了避免网络问题，建议先下载到本地\n",
    "\n",
    "https://huggingface.co/datasets/LooksJuicy/ruozhiba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_path = \"/home/jiangna1/mindnlp_llama_all/alpaca_data.json\" #英文数据集\n",
    "data_path = \"/home/liangdeqi/liudengjin/chinese_data.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看数据具体内容，该数据只包括instruction和output两列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从指定路径加载预训练的分词器，该分词器能将输入文本分割成模型可处理的词元。接着，将填充标记设置为结束标记，这样在处理不同长度的文本序列时，用结束标记来填充额外位置，避免引入额外特殊标记，减少模型学习负担。最后，设置填充方向为右侧，使文本在右侧添加填充标记达到统一长度，维持文本原始顺序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.encode(' ;')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据分为训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_generator(dataset, tokenizer):\n",
    "    for item in dataset:\n",
    "        yield tokenize_function(item, tokenizer)\n",
    "        \n",
    "split_ratio = 0.9\n",
    "split_index = int(len(data) * split_ratio)\n",
    "train_data, val_data = data[:split_index], data[split_index:]\n",
    "\n",
    "train_dataset = ds.GeneratorDataset(\n",
    "    source=lambda: data_generator(train_data, tokenizer), \n",
    "    column_names=[\"input_ids\", \"labels\"]\n",
    ")\n",
    "\n",
    "eval_dataset = ds.GeneratorDataset(\n",
    "    source=lambda: data_generator(val_data, tokenizer), \n",
    "    column_names=[\"input_ids\", \"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看处理后的数据，tokenizer 将输入的文本（prompt）拆分为词片段（tokens），然后将每个词片段映射为对应的 token ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(train_dataset.create_dict_iterator()):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"Sample {i}: Input IDs: {sample['input_ids'][:10]}\") \n",
    "    print(f\"Sample {i}: Labels: {sample['labels'][:10]}\\n\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lora指令微调"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定微调结果输出路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 指定输出路径\n",
    "peft_output_dir = \"/home/liangdeqi/liudengjin/pert_model_Chinese\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载基座模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.transformers import AutoModelForCausalLM,  GenerationConfig\n",
    "\n",
    "ms_base_model = AutoModelForCausalLM.from_pretrained(base_model_path, ms_dtype=ms.float16)\n",
    "ms_base_model.generation_config = GenerationConfig.from_pretrained(base_model_path)\n",
    "ms_base_model.generation_config.pad_token_id = ms_base_model.generation_config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#修改精度，会使训练变慢，但是训练loss下降效果会变好\n",
    "for name, param in ms_base_model.parameters_and_names():\n",
    "    param.set_dtype(ms.float32) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分代码的主要作用是创建一个 LoRA的配置对象 ms_config，大语言模型进行微调时，LoRA 是一种高效的参数微调方法，通过在预训练模型的基础上添加低秩矩阵来减少需要训练的参数数量，从而提高微调效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mindnlp.peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
    "\n",
    "ms_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,#微调任务的类型\n",
    "    #指定需要应用 LoRA 调整的目标模块\n",
    "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    inference_mode=False,  \n",
    "    r=8,                   \n",
    "    lora_alpha=32,         \n",
    "    lora_dropout=0.1       \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于给定的基础模型和 LoRA 配置创建一个可进行参数高效微调的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_model = get_peft_model(ms_base_model, ms_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 微调\n",
    "通过 BertForSequenceClassification 构建用于情感分类的 BERT 模型，加载预训练权重，设置情感三分类的超参数自动构建模型。后面对模型采用自动混合精度操作，提高训练的速度，然后实例化优化器，紧接着实例化评价指标，设置模型训练的权重保存策略，最后就是构建训练器，模型开始训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练参数的设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.engine import TrainingArguments, Trainer\n",
    "\n",
    "num_train_epochs = 10\n",
    "fp16 = True\n",
    "overwrite_output_dir = True\n",
    "per_device_train_batch_size = 2\n",
    "per_device_eval_batch_size = 4\n",
    "gradient_accumulation_steps = 16\n",
    "gradient_checkpointing = True\n",
    "evaluation_strategy = \"steps\"\n",
    "learning_rate = 1e-5\n",
    "lr_scheduler_type = \"cosine\" \n",
    "weight_decay = 0.01\n",
    "warmup_ratio = 0.1\n",
    "max_grad_norm = 0.3\n",
    "group_by_length = False \n",
    "auto_find_batch_size = False\n",
    "save_steps = 50 \n",
    "logging_strategy = \"steps\"\n",
    "logging_steps = 150                \n",
    "load_best_model_at_end = True \n",
    "packing = False\n",
    "save_total_limit = 3\n",
    "neftune_noise_alpha = 5 \n",
    "eval_steps = 10\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=peft_output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    load_best_model_at_end=load_best_model_at_end,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    evaluation_strategy=evaluation_strategy,\n",
    "    eval_steps=eval_steps,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    auto_find_batch_size=auto_find_batch_size,\n",
    "    save_total_limit=save_total_limit,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    save_steps=save_steps,\n",
    "    logging_strategy=logging_strategy,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=ms_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=training_arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in trainer.model.parameters_and_names():\n",
    "    param.set_dtype(ms.float16)\n",
    "trainer.model.save_pretrained(peft_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后存储还是存储为 16 位浮点数，保存训练后的 LoRA 模型保存到指定的输出目录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，lora微调后保存的并不是完整的参数，在推理时，需要将保存的 LoRA 参数加载到原预训练模型中，合并后得到完整的模型，然后进行推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用PeftModel进行配置和参数合并，最后将模型设置为评估模式，进行推理任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将 LoRA微调后的参数加载到预训练模型中\n",
    "from mindnlp.peft import PeftModel\n",
    "model = PeftModel.from_pretrained(ms_base_model, peft_output_dir)\n",
    "model = model.merge_and_unload()\n",
    "print('model merge succeeded')\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个函数，用于根据用户输入的问题生成相应的回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "\n",
    "def generate_response(question, model, tokenizer, max_length=256):\n",
    "    prompt = f\"以下是用户和助手之间的问答。\\n问：{question}\\n答：\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"ms\", padding=True, truncation=True, max_length=512)\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        # temperature=0.7,\n",
    "        # top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        no_repeat_ngram_size=3,\n",
    "        max_length=max_length,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    response = response.split(\"Answer：\")[-1].strip()\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"如何保持清醒?\"\n",
    "response = generate_response(question, model, tokenizer)\n",
    "\n",
    "print(f\"User: {question}\")\n",
    "print(f\"LLAMA: {response}\")"
   ]
  }
 ],
 "metadata": {
  "AIGalleryInfo": {
   "item_id": "5443b528-0dd5-4909-ac4f-1c9cf839e2aa"
  },
  "flavorInfo": {
   "architecture": "X86_64",
   "category": "GPU"
  },
  "imageInfo": {
   "id": "e1a07296-22a8-4f05-8bc8-e936c8e54202",
   "name": "mindspore1.7.0-cuda10.1-py3.7-ubuntu18.04"
  },
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
