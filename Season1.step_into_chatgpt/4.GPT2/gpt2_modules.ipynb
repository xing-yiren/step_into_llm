{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# GPT2 Masked Multi-head Self-attention详解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "该实验可进行在线体验，在线体验链接（https://pangu.huaweicloud.com/gallery/asset-detail.html?id=6253fbfb-afe6-4727-bca5-5fc726541ab2\n",
    "）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 环境配置\n",
    ">\n",
    ">此为在线运行平台配置python3.9的指南，如在其他环境平台运行案例，请根据实际情况修改如下代码\n",
    ">\n",
    "\n",
    "1. 配置python3.9环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "!/home/ma-user/anaconda3/bin/conda create -n python-3.9.0 python=3.9.0 -y --override-channels --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n",
    "!/home/ma-user/anaconda3/envs/python-3.9.0/bin/pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "data = {\n",
    "   \"display_name\": \"python-3.9.0\",\n",
    "   \"env\": {\n",
    "      \"PATH\": \"/home/ma-user/anaconda3/envs/python-3.9.0/bin:/home/ma-user/anaconda3/envs/python-3.7.10/bin:/modelarts/authoring/notebook-conda/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/ma-user/modelarts/ma-cli/bin:/home/ma-user/modelarts/ma-cli/bin\"\n",
    "   },\n",
    "   \"language\": \"python\",\n",
    "   \"argv\": [\n",
    "      \"/home/ma-user/anaconda3/envs/python-3.9.0/bin/python\",\n",
    "      \"-m\",\n",
    "      \"ipykernel\",\n",
    "      \"-f\",\n",
    "      \"{connection_file}\"\n",
    "   ]\n",
    "}\n",
    "\n",
    "if not os.path.exists(\"/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/\"):\n",
    "    os.mkdir(\"/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/\")\n",
    "\n",
    "with open('/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/kernel.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "***注：以上代码执行完成后，需点击左上角或右上角将kernel更换为python-3.9.0***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://mindspore-demo.obs.cn-north-4.myhuaweicloud.com/imgs/ai-gallery/change-kernel.PNG\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "2. 安装mindspore2.2.12、indNLP及相关依赖，MindNLP官方仓详见：MindNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "!pip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.2.14/MindSpore/unified/x86_64/mindspore-2.2.14-cp39-cp39-linux_x86_64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install tokenizers==0.15.0 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!wget https://repo.mindspore.cn/mindspore-lab/mindnlp/daily/202402/20240229/master_20240229160016_c5444092d8cfe47d73e292f25d9a9a56fb04828a_newest/any/mindnlp-0.2.0.20240229-py3-none-any.whl\n",
    "!pip install mindnlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "***注：执行如上命令完成安装后，请点击上方的restart kernel图标重启kernel，再进行实验***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# code from mindnlp and huggingface transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mindspore\n",
    "from mindspore import nn, ops, Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## GPT-2 Self-attention: 1- Creating queries, keys, and values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "![gpt2-self-attention-3.png](https://jalammar.github.io/images/gpt2/gpt2-self-attention-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "seq_len = 10\n",
    "embed_dim = 768\n",
    "\n",
    "# input x: (1, 10, 768)\n",
    "x = Tensor(np.random.randn(batch_size, seq_len, embed_dim), mindspore.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mindnlp._legacy.functional import split\n",
    "from mindnlp.transformers.ms_utils import Conv1D\n",
    "\n",
    "# query = Wq * X, key = Wk * X, value = Wv * X\n",
    "# c_attn: (1, 10, 768*3) --> query, key, value: (1, 10, 768), (1, 10, 768), (1, 10, 768)\n",
    "c_attn = Conv1D(3 * embed_dim, embed_dim)\n",
    "query, key, value = split(c_attn(x), embed_dim, axis=2)\n",
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "![gpt2-self-attention-split-attention-heads-1.png](https://jalammar.github.io/images/gpt2/gpt2-self-attention-split-attention-heads-1.png)\n",
    "\n",
    "![gpt2-self-attention-split-attention-heads-2.png](https://jalammar.github.io/images/gpt2/gpt2-self-attention-split-attention-heads-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_heads(tensor, num_heads, attn_head_size):\n",
    "    \"\"\"\n",
    "    Splits hidden_size dim into attn_head_size and num_heads\n",
    "    \"\"\"\n",
    "    # (batch_size, seq_len, hidden_size) --> (batch_size, seq_len, num_heads, attn_head_size)\n",
    "    new_shape = tensor.shape[:-1] + (num_heads, attn_head_size)\n",
    "    tensor = tensor.view(new_shape)\n",
    "    # (batch_size, seq_len, num_heads, attn_head_size) --> (batch_size, num_heads, seq_len, attn_head_size)\n",
    "    return ops.transpose(tensor, (0, 2, 1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_heads = 12\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "# (1, 10, 768) --> (1, 10, 12, 64) --> (1, 12, 10, 64)\n",
    "query = split_heads(query, num_heads, head_dim)\n",
    "key = split_heads(key, num_heads, head_dim)\n",
    "value = split_heads(value, num_heads, head_dim)\n",
    "\n",
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## GPT-2 Self-attention: 2- Scoring\n",
    "\n",
    "![gpt2-self-attention-scoring.png](https://jalammar.github.io/images/gpt2/gpt2-self-attention-scoring.png)\n",
    "\n",
    "![](https://jalammar.github.io/images/gpt2/gpt2-self-attention-scoring-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# qk点积\n",
    "# q: (1, 12, 10, 64), k^T: (1, 12, 64, 10)\n",
    "# attn_weights: (1, 12, 10, 10)\n",
    "attn_weights = ops.matmul(query, key.swapaxes(-1, -2))\n",
    "\n",
    "attn_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](https://jalammar.github.io/images/gpt2/transformer-decoder-attention-mask-dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# diagonal matrix to implement masked multi-head attention\n",
    "# To ensure not to attend to future information\n",
    "max_positions = seq_len\n",
    "\n",
    "bias = Tensor(np.tril(np.ones((max_positions, max_positions))).reshape(\n",
    "              (1, 1, max_positions, max_positions)), mindspore.bool_)\n",
    "bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "![](https://jalammar.github.io/images/gpt2/queries-keys-attention-mask.png)\n",
    "\n",
    "![](https://jalammar.github.io/images/gpt2/transformer-attention-mask.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mindnlp._legacy.functional import where, softmax\n",
    "\n",
    "attn_weights = attn_weights / ops.sqrt(ops.scalar_to_tensor(value.shape[-1]))\n",
    "query_length, key_length = query.shape[-2], key.shape[-2]\n",
    "causal_mask = bias[:, :, key_length - query_length: key_length, :key_length].bool()\n",
    "mask_value = Tensor(np.finfo(np.float32).min, dtype=attn_weights.dtype)\n",
    "attn_weights = where(causal_mask, attn_weights, mask_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.finfo(np.float32).min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "![](https://jalammar.github.io/images/gpt2/transformer-attention-masked-scores-softmax.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attn_weights = softmax(attn_weights, axis=-1)\n",
    "attn_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attn_weights[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "![](https://jalammar.github.io/images/gpt2/gpt2-self-attention-multihead-sum-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attn_output = ops.matmul(attn_weights, value)\n",
    "\n",
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## GPT-2 Self-attention: 3.5- Merge attention heads\n",
    "\n",
    "![](https://jalammar.github.io/images/gpt2/gpt2-self-attention-merge-heads-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_heads(tensor, num_heads, attn_head_size):\n",
    "    \"\"\"\n",
    "    Merges attn_head_size dim and num_attn_heads dim into hidden_size\n",
    "    \"\"\"\n",
    "    # (batch_size, num_heads, seq_len, attn_head_size) --> (batch_size, seq_len, num_heads, seq_len)\n",
    "    tensor = ops.transpose(tensor, (0, 2, 1, 3))\n",
    "    new_shape = tensor.shape[:-2] + (num_heads * attn_head_size,)\n",
    "    return tensor.view(new_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (1, 12, 10, 64) --> (1, 10, 12, 64) --> (1, 10, 768)\n",
    "attn_output = merge_heads(attn_output, num_heads, head_dim)\n",
    "\n",
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## GPT-2 Self-attention: 4- Projecting\n",
    "\n",
    "![](https://jalammar.github.io/images/gpt2/gpt2-self-attention-project-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_proj = Conv1D(embed_dim, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attn_output = c_proj(attn_output)\n",
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "AIGalleryInfo": "",
  "flavorInfo": {
   "architecture": "X86_64",
   "category": "GPU"
  },
  "imageInfo": {
   "id": "e1a07296-22a8-4f05-8bc8-e936c8e54202",
   "name": "mindspore1.7.0-cuda10.1-py3.7-ubuntu18.04"
  },
  "kernelspec": {
   "display_name": "ms2.2.14",
   "language": "python",
   "name": "ms2.2.14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
